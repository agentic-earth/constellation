{
    "title": "Investigating the fidelity of explainable artificial intelligence\n  methods for applications of convolutional neural networks in geoscience",
    "authors": [
        "Antonios Mamalakis",
        "Elizabeth A. Barnes",
        "Imme Ebert-Uphoff"
    ],
    "abstract": "  Convolutional neural networks (CNNs) have recently attracted great attention\nin geoscience due to their ability to capture non-linear system behavior and\nextract predictive spatiotemporal patterns. Given their black-box nature\nhowever, and the importance of prediction explainability, methods of\nexplainable artificial intelligence (XAI) are gaining popularity as a means to\nexplain the CNN decision-making strategy. Here, we establish an intercomparison\nof some of the most popular XAI methods and investigate their fidelity in\nexplaining CNN decisions for geoscientific applications. Our goal is to raise\nawareness of the theoretical limitations of these methods and gain insight into\nthe relative strengths and weaknesses to help guide best practices. The\nconsidered XAI methods are first applied to an idealized attribution benchmark,\nwhere the ground truth of explanation of the network is known a priori, to help\nobjectively assess their performance. Secondly, we apply XAI to a\nclimate-related prediction setting, namely to explain a CNN that is trained to\npredict the number of atmospheric rivers in daily snapshots of climate\nsimulations. Our results highlight several important issues of XAI methods\n(e.g., gradient shattering, inability to distinguish the sign of attribution,\nignorance to zero input) that have previously been overlooked in our field and,\nif not considered cautiously, may lead to a distorted picture of the CNN\ndecision-making strategy. We envision that our analysis will motivate further\ninvestigation into XAI fidelity and will help towards a cautious implementation\nof XAI in geoscience, which can lead to further exploitation of CNNs and deep\nlearning for prediction problems.\n",
    "published_date": "2022-02-07T18:47:15Z",
    "doi": "10.1175/AIES-D-22-0012.1",
    "pdf_url": "http://arxiv.org/pdf/2202.03407v2"
}